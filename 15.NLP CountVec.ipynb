{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a61a0619-ef4e-4bc0-b69c-280e50ebf625",
   "metadata": {},
   "source": [
    "NLTK (Natural Language Toolkit) is a widely used library for NLP tasks.To perform tokenization using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf736f9c-0933-4f2f-8ea6-ad2d86d2ac67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f2b684f-a51f-4ede-946a-9d34eefb1c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['change', 'changing', 'changes', 'changed']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = ['change','changing','changes','changed']\n",
    "word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e2aec5-811e-4cd2-a963-aca3af1579d5",
   "metadata": {},
   "source": [
    "PorterStemmer is a stemming algorithm Stemming is a text normalization technique in NLP. It's the process of reducing words to their root or base form, which helps in simplifying the words and making it easier to analyze text. For example, stemming would reduce words like \"running,\" \"ran,\" and \"runner\" to the common base form \"run.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "483aaa36-0871-4157-8d71-1c5227dbb9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e70918b4-660a-4170-a985-6c4f598b6792",
   "metadata": {},
   "outputs": [],
   "source": [
    "p =PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df8c72cf-5d29-4eec-b080-6cb25e8e4fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "change chang\n",
      "changing chang\n",
      "changes chang\n",
      "changed chang\n"
     ]
    }
   ],
   "source": [
    "for w in word:\n",
    "    print(w,p.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a19a842-1208-4eeb-a764-43fc22fa097f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = 'I want to change the world immediately if world changed my career by the changing abcd introduction or lemmatization'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82ea7b2b-4b9f-49d6-b1bf-30850d32fb27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\MD.NAIM\n",
      "[nltk_data]     HOSSAIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d871eef5-37a5-4d68-af1a-bfd692296453",
   "metadata": {},
   "outputs": [],
   "source": [
    "toke = word_tokenize(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09e1ad14-ab43-4335-854d-ab0f84e165a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'want',\n",
       " 'to',\n",
       " 'change',\n",
       " 'the',\n",
       " 'world',\n",
       " 'immediately',\n",
       " 'if',\n",
       " 'world',\n",
       " 'changed',\n",
       " 'my',\n",
       " 'career',\n",
       " 'by',\n",
       " 'the',\n",
       " 'changing',\n",
       " 'abcd',\n",
       " 'introduction',\n",
       " 'or',\n",
       " 'lemmatization']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3fb72c7-e500-4d22-90bb-b04ba5af67ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I i\n",
      "want want\n",
      "to to\n",
      "change chang\n",
      "the the\n",
      "world world\n",
      "immediately immedi\n",
      "if if\n",
      "world world\n",
      "changed chang\n",
      "my my\n",
      "career career\n",
      "by by\n",
      "the the\n",
      "changing chang\n",
      "abcd abcd\n",
      "introduction introduct\n",
      "or or\n",
      "lemmatization lemmat\n"
     ]
    }
   ],
   "source": [
    "for w in toke:\n",
    "    print(w,p.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe3bb29-da93-4705-9f2a-458e5449ae82",
   "metadata": {},
   "source": [
    "Lemmatization is a text normalization technique.It reduces words to their base or dictionary form, known as the \"lemma.\" Unlike stemming, which just removes prefixes and suffixes to get to a common base form, produces a valid word that exists in the language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85f645ea-0575-428e-9aaa-719f7472e156",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75b388e0-f3a5-4164-aae2-879e47be2bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\MD.NAIM\n",
      "[nltk_data]     HOSSAIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9fd9545e-1eea-403f-9cd2-88e17e5293b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98bce7a8-37d1-4857-9854-3f033031b472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'want',\n",
       " 'to',\n",
       " 'change',\n",
       " 'the',\n",
       " 'world',\n",
       " 'immediately',\n",
       " 'if',\n",
       " 'world',\n",
       " 'changed',\n",
       " 'my',\n",
       " 'career',\n",
       " 'by',\n",
       " 'the',\n",
       " 'changing',\n",
       " 'abcd',\n",
       " 'introduction',\n",
       " 'or',\n",
       " 'lemmatization']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a16c3a3a-6a0d-46be-b983-b907bdeffebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I I\n",
      "want want\n",
      "to to\n",
      "change change\n",
      "the the\n",
      "world world\n",
      "immediately immediately\n",
      "if if\n",
      "world world\n",
      "changed changed\n",
      "my my\n",
      "career career\n",
      "by by\n",
      "the the\n",
      "changing changing\n",
      "abcd abcd\n",
      "introduction introduction\n",
      "or or\n",
      "lemmatization lemmatization\n"
     ]
    }
   ],
   "source": [
    "for w in toke:\n",
    "    print(w,le.lemmatize(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3dc0dfec-2bb8-4427-a00f-2acdccc93a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chang'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.lemmatize('changs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5bcfa9-dc77-4b03-863e-facc043cd25a",
   "metadata": {},
   "source": [
    "word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22c8b235-b75b-44a7-87df-9c7852d4fdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0f625521-600a-4090-873d-eef26c358c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'I am from aiqust Intelligence. I am learning NLP. It is facinating!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d948808a-fa1d-4a0b-9f4c-4b28b6e3d125",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens = word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bd3ed0f7-0e17-4eb0-a0b6-188a67aef904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'am',\n",
       " 'from',\n",
       " 'aiqust',\n",
       " 'Intelligence',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'learning',\n",
       " 'NLP',\n",
       " '.',\n",
       " 'It',\n",
       " 'is',\n",
       " 'facinating',\n",
       " '!']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "12fcdd87-e58f-4b6f-a9ad-e344bbbb3a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tokens =sent_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4769c48f-eb4c-4deb-949c-bd9c80c0e4a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am from aiqust Intelligence.', 'I am learning NLP.', 'It is facinating!']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9135a498-e4a7-4d3c-8d0e-98b657283c97",
   "metadata": {},
   "source": [
    "spaCy is a another powerful library for NLP, to install spaCy, \n",
    "\n",
    "1.Part-of-Speech Tagging: (e.g., noun, verb, adjective) to words in a text, helping you analyze the grammatical structure.\r\n",
    "2.\r\n",
    "Named Entity Recognition (NER): SpaCy can identify and classify named entities in text, such as names of people, organizations, locations, dates, and more\n",
    "3.\r\n",
    "\r\n",
    "Dependency Parsing: It performs dependency parsing to analyze the grammatical structure of a sentence by representing the relationships between words as a dependency tr\n",
    "4.e.\r\n",
    "\r\n",
    "Text Classification: SpaCy supports text classification tasks, allowing you to build models to categorize text into different classes or labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "11afc3f7-33f7-4565-8183-4a056d29126e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "645727c2-4ba9-4c5a-8675-ccd9775c33c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'from', 'aiQust', 'Intelligence', '.', 'I', 'am', 'learning', 'NLP', '.', 'It', 'is', 'Fascinating']\n"
     ]
    }
   ],
   "source": [
    "# python -m spacy download en_core_web_sm (need this command for run)\n",
    "nlp = spacy.load('en_core_web_sm') #load english language model\n",
    "sentence = 'I am from aiQust Intelligence. I am learning NLP. It is Fascinating'\n",
    "doc = nlp(sentence)\n",
    "word_tokens = [token.text for token in doc]\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c810e01b-9a81-48ca-bd2f-3f609524305c",
   "metadata": {},
   "source": [
    "Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510240ac-7868-48b1-a804-435ffdb846cf",
   "metadata": {},
   "source": [
    "Transfomers is a library built by Hugging Face that provide state of the art pre traine model for nlpl,pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a5a55e54-df31-490d-89f4-e5a404a86d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer  # more used in neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c5f6c569-4dba-48af-9a95-a21b6d4af928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'am',\n",
       " 'from',\n",
       " 'raj',\n",
       " '##shah',\n",
       " '##i',\n",
       " '.',\n",
       " 'i',\n",
       " 'lives',\n",
       " 'in',\n",
       " 'pa',\n",
       " '##bn',\n",
       " '##a',\n",
       " 'town']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "sentence = 'I am from Rajshahi. I lives in pabna town'\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8c0a94-ef5a-4557-87df-b90daeb821a4",
   "metadata": {},
   "source": [
    "Named Entity Tokenization using NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01da322-f39d-4bb6-a4fa-e4d473055e3a",
   "metadata": {},
   "source": [
    "to perform named entity tokenization using NLTK .It function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "194d111a-7f94-4585-a268-9586a9e345f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\MD.NAIM HOSSAIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1a9dc4b4-33d4-4515-9ee8-78954b052cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to C:\\Users\\MD.NAIM\n",
      "[nltk_data]     HOSSAIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to C:\\Users\\MD.NAIM\n",
      "[nltk_data]     HOSSAIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker') #download the required resouces\n",
    "nltk.download('words') #for word corpus\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "sen = 'I am from Rajshahi. I lives in pabna town'\n",
    "tokens= word_tokenize(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a9d8b6c3-6cf2-43f4-9046-3d6a2217eeb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'from', 'Rajshahi', '.', 'I', 'lives', 'in', 'pabna', 'town']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1e2e4d8d-a271-4feb-8589-1c0d3b0d2a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('am', 'VBP'), ('from', 'IN'), ('Rajshahi', 'NNP'), ('.', '.'), ('I', 'PRP'), ('lives', 'VBZ'), ('in', 'IN'), ('pabna', 'NN'), ('town', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "pos_tags = pos_tag(tokens)\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6dd207c5-7410-4210-adc0-1a6c55a8ec1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,464.0,168.0\" width=\"464px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"8.62069%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">I</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PRP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"4.31034%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"8.62069%\" x=\"8.62069%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">am</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"12.931%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"10.3448%\" x=\"17.2414%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">from</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"22.4138%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"17.2414%\" x=\"27.5862%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">GPE</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Rajshahi</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"36.2069%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"5.17241%\" x=\"44.8276%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"47.4138%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"8.62069%\" x=\"50%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">I</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PRP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"54.3103%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"12.069%\" x=\"58.6207%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">lives</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBZ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"64.6552%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"6.89655%\" x=\"70.6897%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">in</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"74.1379%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"12.069%\" x=\"77.5862%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">pabna</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"83.6207%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"10.3448%\" x=\"89.6552%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">town</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"94.8276%\" y1=\"19.2px\" y2=\"48px\" /></svg>"
      ],
      "text/plain": [
       "Tree('S', [('I', 'PRP'), ('am', 'VBP'), ('from', 'IN'), Tree('GPE', [('Rajshahi', 'NNP')]), ('.', '.'), ('I', 'PRP'), ('lives', 'VBZ'), ('in', 'IN'), ('pabna', 'NN'), ('town', 'NN')])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_tags = ne_chunk(pos_tags)\n",
    "ner_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5b8f77dd-90a1-47ee-addb-ef6adc464f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Rajshahi']\n"
     ]
    }
   ],
   "source": [
    "named_entity_tokens=[]\n",
    "for chunk in ner_tags:\n",
    "    if hasattr(chunk,'label'):  #hasattr(object,attribute)\n",
    "        named_entity_tokens.append(' '.join(c[0] for c in chunk))\n",
    "print(named_entity_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2403e6f8-4aee-4bbe-8f33-c08213a5a7af",
   "metadata": {},
   "source": [
    "Text Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ca859278-df34-4857-a33a-3cb859db1244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('NLP data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1422f878-af8c-4ca3-a996-5dc928d4719e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I love Bangladesh</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Could you give me an iphone?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hello how are you?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I want to talk you.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           test  class\n",
       "0             I love Bangladesh      1\n",
       "1  Could you give me an iphone?      0\n",
       "2            Hello how are you?      1\n",
       "3           I want to talk you.      1"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5e1a0c9c-e3e3-4002-84e0-cbd600c30bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "68c685c0-82b6-4bef-a188-6a85fd1b3018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count vectorizer\n",
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3abe7471-95d9-4be6-a505-f6915d3ce48b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x14 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 16 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_x = cv.fit_transform(df['test'])\n",
    "cv_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b24d3039-fefb-4f3a-809f-c4a01a9bbb90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1],\n",
       "       [0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_x.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "37bffd91-6010-4d6a-b337-f57a1737ef42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['an', 'are', 'bangladesh', 'could', 'give', 'hello', 'how',\n",
       "       'iphone', 'love', 'me', 'talk', 'to', 'want', 'you'], dtype=object)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "26d51be9-6659-4a62-98a2-f2c563b033cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text data column ke row brabr set up dibe\n",
    "cv_df = pd.DataFrame(cv_x.toarray(),columns =cv.get_feature_names_out(),index=df['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "83e1563b-44a3-4d99-b24d-f82e64639571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>an</th>\n",
       "      <th>are</th>\n",
       "      <th>bangladesh</th>\n",
       "      <th>could</th>\n",
       "      <th>give</th>\n",
       "      <th>hello</th>\n",
       "      <th>how</th>\n",
       "      <th>iphone</th>\n",
       "      <th>love</th>\n",
       "      <th>me</th>\n",
       "      <th>talk</th>\n",
       "      <th>to</th>\n",
       "      <th>want</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I love Bangladesh</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Could you give me an iphone?</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hello how are you?</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I want to talk you.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              an  are  bangladesh  could  give  hello  how  \\\n",
       "test                                                                         \n",
       "I love Bangladesh              0    0           1      0     0      0    0   \n",
       "Could you give me an iphone?   1    0           0      1     1      0    0   \n",
       "Hello how are you?             0    1           0      0     0      1    1   \n",
       "I want to talk you.            0    0           0      0     0      0    0   \n",
       "\n",
       "                              iphone  love  me  talk  to  want  you  \n",
       "test                                                                 \n",
       "I love Bangladesh                  0     1   0     0   0     0    0  \n",
       "Could you give me an iphone?       1     0   1     0   0     0    1  \n",
       "Hello how are you?                 0     0   0     0   0     0    1  \n",
       "I want to talk you.                0     0   0     1   1     1    1  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5217ee15-00e0-4f97-abac-7e2f6da562f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df =pd.DataFrame(cv_x.toarray(),columns =cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "52b6c52d-a8ca-4f49-be31-3b466883b4b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>an</th>\n",
       "      <th>are</th>\n",
       "      <th>bangladesh</th>\n",
       "      <th>could</th>\n",
       "      <th>give</th>\n",
       "      <th>hello</th>\n",
       "      <th>how</th>\n",
       "      <th>iphone</th>\n",
       "      <th>love</th>\n",
       "      <th>me</th>\n",
       "      <th>talk</th>\n",
       "      <th>to</th>\n",
       "      <th>want</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   an  are  bangladesh  could  give  hello  how  iphone  love  me  talk  to  \\\n",
       "0   0    0           1      0     0      0    0       0     1   0     0   0   \n",
       "1   1    0           0      1     1      0    0       1     0   1     0   0   \n",
       "2   0    1           0      0     0      1    1       0     0   0     0   0   \n",
       "3   0    0           0      0     0      0    0       0     0   0     1   1   \n",
       "\n",
       "   want  you  \n",
       "0     0    0  \n",
       "1     0    1  \n",
       "2     0    1  \n",
       "3     1    1  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afabe3d7-ff8b-4104-83d7-14aeea311a1f",
   "metadata": {},
   "source": [
    "TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e0492177-7d98-4b60-80e1-2acf325117a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf= TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1d0a16ee-307e-4d2d-9750-2dddf34da787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x14 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 16 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_z =tf.fit_transform(df['test'])\n",
    "tf_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "baeb4348-5ab0-4b74-b0ab-fcf0efbc64b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>an</th>\n",
       "      <th>are</th>\n",
       "      <th>bangladesh</th>\n",
       "      <th>could</th>\n",
       "      <th>give</th>\n",
       "      <th>hello</th>\n",
       "      <th>how</th>\n",
       "      <th>iphone</th>\n",
       "      <th>love</th>\n",
       "      <th>me</th>\n",
       "      <th>talk</th>\n",
       "      <th>to</th>\n",
       "      <th>want</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I love Bangladesh</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Could you give me an iphone?</th>\n",
       "      <td>0.430037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.430037</td>\n",
       "      <td>0.430037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.430037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.430037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.274487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hello how are you?</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.541736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.541736</td>\n",
       "      <td>0.541736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.345783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I want to talk you.</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.541736</td>\n",
       "      <td>0.541736</td>\n",
       "      <td>0.541736</td>\n",
       "      <td>0.345783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    an       are  bangladesh     could  \\\n",
       "test                                                                     \n",
       "I love Bangladesh             0.000000  0.000000    0.707107  0.000000   \n",
       "Could you give me an iphone?  0.430037  0.000000    0.000000  0.430037   \n",
       "Hello how are you?            0.000000  0.541736    0.000000  0.000000   \n",
       "I want to talk you.           0.000000  0.000000    0.000000  0.000000   \n",
       "\n",
       "                                  give     hello       how    iphone  \\\n",
       "test                                                                   \n",
       "I love Bangladesh             0.000000  0.000000  0.000000  0.000000   \n",
       "Could you give me an iphone?  0.430037  0.000000  0.000000  0.430037   \n",
       "Hello how are you?            0.000000  0.541736  0.541736  0.000000   \n",
       "I want to talk you.           0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "                                  love        me      talk        to  \\\n",
       "test                                                                   \n",
       "I love Bangladesh             0.707107  0.000000  0.000000  0.000000   \n",
       "Could you give me an iphone?  0.000000  0.430037  0.000000  0.000000   \n",
       "Hello how are you?            0.000000  0.000000  0.000000  0.000000   \n",
       "I want to talk you.           0.000000  0.000000  0.541736  0.541736   \n",
       "\n",
       "                                  want       you  \n",
       "test                                              \n",
       "I love Bangladesh             0.000000  0.000000  \n",
       "Could you give me an iphone?  0.000000  0.274487  \n",
       "Hello how are you?            0.000000  0.345783  \n",
       "I want to talk you.           0.541736  0.345783  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_df =pd.DataFrame(tf_z.toarray(),columns =tf.get_feature_names_out(),index =df['test'])\n",
    "cv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5a6675-c3d4-4afd-a802-e4bcb66dea9e",
   "metadata": {},
   "source": [
    "word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0ae156f1-4f17-4afb-9b63-43145ef1d5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bb9c2fa0-30f6-4f8d-95d8-e25d9bb3970f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['I', 'love', 'Bangladesh'],\n",
       " ['Could', 'you', 'give', 'me', 'an', 'iphone', '?'],\n",
       " ['Hello', 'how', 'are', 'you', '?'],\n",
       " ['I', 'want', 'to', 'talk', 'you', '.']]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vector =[nltk.word_tokenize(test) for test in df['test']]\n",
    "text_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1bd24c71-ef50-4c00-890d-d7c84baf9472",
   "metadata": {},
   "outputs": [],
   "source": [
    "model= Word2Vec(text_vector,min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e0b20eb0-1764-47a6-87c9-bbfe54e38d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('an', 0.17826788127422333),\n",
       " ('I', 0.16072483360767365),\n",
       " ('give', 0.10560768842697144),\n",
       " ('how', 0.09215974807739258),\n",
       " ('iphone', 0.048910051584243774),\n",
       " ('are', 0.027008380740880966),\n",
       " ('Could', 0.007729308679699898),\n",
       " ('you', -0.03771637752652168),\n",
       " ('.', -0.04552281275391579),\n",
       " ('talk', -0.0464920774102211)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('want')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49c6a43-f701-4923-9cdb-647c75f27b50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
